# 1.3.2 梯度下降优化

所以，在上一个视频中，我们学到了两件非常重要的事情。

首先，我们了解到线性回归只不过是一种非常特殊的神经网络。

即，它是由一个线性神经元组成的网络。

还有更先进的神经网络称为前馈网络，可以在网络各层之间组织多层的信息处理，从输入到输出。

我们还说过，使用一种名为梯度下降的方法，可以将这种网络中所有神经元的权重 W 拟合到某些训练数据。

现在，因为线性回归是一个非常特殊的神经网络，让我们首先看一下，对一个实现线性回归的神经元，它如何工作。

要为此，让我们再回到上一个视频中的示例。

回顾一下，我们有一组输入 x0 到 xN，它们由线性神经元变换，也就是说，我们在它们的输出中使用线性激活函数 h(x)，依赖输入和权重W.

现在，我们如何微调参数 W 来提供一些数据？那么，对于线性回归，我们已经知道了答案。

它是由我们上周讨论的正规方程给出的。

让我们简要回顾一下上周我们所涵盖的数学。

我们说最佳权重应该是这样，均方误差 MSE train 对于这些最优权重的梯度应该消失。

在一些代数之后，这产生了称为正规方程的，最佳权重向量的解析表达式。

对于许多情况，这是一个很好的解决方案

但在许多其他情况下，并非如此简单。

首先，这个精确解仅适用于具有二次损失函数的线性回归。

更改损失函数，然后就没有精确解了。

其次，正规方程在这里涉及求逆，其具有`O(n^3)`的高复杂度成本，其可以通过使用专用算法进行矩阵求逆，而减少到像`O(n^2.4)`。

但是，当你的数据矩阵 X 具有数十或数十万的列数时，这仍然是复杂性的高度计算，可能导致计算速度非常慢。

事实证明，梯度下降的方法是一种选择方法，可用于这种情况，以及如果我们使用非线性回归而不是线性回归，可能需要的其他损失函数。

梯度下降只是丢弃了正规方程并退后一步。

如上所述，损失函数的梯度应该为零的最佳点，梯度下降在步骤中灵活地找到该最佳点。

让我们看看它如何在一个变量的简单示例上工作。

让我们将这个函数的参数称为 theta ，以及我们想要最小化的函数是`g(theta )`。

如果我们处理线性回归，则g(theta )将是损失的均方，而 theta 将是回归参数。

但我想在这里使用更一般的符号。

那么它是怎样工作的？我们从一些初始猜测 theta0 开始，并计算此时函数 g 的导数。

该导数由此红线给出，在此示例中为负。

然后，我们通过将我们的初始值移动一个量，它等于负的参数 eta 乘以 theta0 处的函数的梯度，来更新 theta 的值。

参数 eta 称为学习率，它在梯度下降法中起着非常重要的作用。

让我们看看这个等式意味着什么。

因为 eta 是正的，而 theta 的梯度是负的，并且由于这里是负的，这整个式子的净值，包括减号，将是正的。

这正是我们想要的，因为最小值在我们最初的猜测 theta0 的右边。

但如果它就在它的右侧，就像这里一样，那么这个术语就是负数，并且初始猜测会在第一次迭代时向左移动。

因此，这里的负号是正确的，因为它意味着为了使函数最小化，我们必须向与其梯度相反的方向前进，这指向该函数的最大方向。

学习率 eta 只是我们向这个方向移动的数量。

如果我们采用更大的 eta，你沿着这条线移动更多，它会给你梯度下降方法的下一个迭代。

现在，之后所做的只是重复这第一步。

同样，使用新的初始猜测 θ1 并且使用相同的等式和相同的学习率 eta 来完成 θ 的当前最佳值的更新。

新的候选解决方案 theta 2 现在更接近真正的最小 theta。

然后该过程继续，直到该函数以给定的容差变化停止。

这意味着算法已经收敛。

好的，这是对算法如何在一维上工作的解释。

当存在许多维度时，该算法遵循相同的规则，沿着每个维度执行数据的梯度下降。

让我向你展示一个简单的动画，了解梯度下降如何适用于二维损失函数。

正如你所看到的，这里显示的是初始点，因为红点在几个优雅的步骤中逐渐下降到函数的真正最小值。

所有这些在实践中如何运作，取决于我们想要最小化的函数和学习率 eta。

例如，我们的学习率可能太低，因此，要找到最小值，需要采取太多步骤。

另一个例子，学习率可能太高。

在这种情况下，解决方案甚至可能那样发散，像这样。

对于较大的学习率，你的迭代每次都会反弹，并且每次反弹实际上会变大。

这将导致解决方案在数字上发散。

当我们想要最小化的函数是非凸的时，使用梯度下降会出现其他微妙的点。

也就是说，它有不止一个最小值。

在这里显示的场景中，使用初始猜测结果，梯度下降可能很难找到 θ 的全局最小值，因为最有可能的是，它将落在此处所示的局部最小值。

稍后，在这个可视化中，我们将讨论如何将学习率 eta，变成迭代次数的函数。

梯度下降法可用于这种非凸优化问题。

我们还将看到，相同的技术如何有助于优化最优值，在某种意义上，最后一个函数具有几乎平的平台，就像这里的函数值几乎恒定一样。

但是现在，我们将考虑具有恒定学习率 eta 的梯度下降方法的基本版本。

因此，我们理解了，梯度下降如何适用于实现线性回归的一个线性神经元的简单情况。

现在，具有更多节点的通用神经网络又如何呢？通用神经网络具有许多自由参数，即每个节点的一组参数。

这种情况下梯度下降也可以吗？正如我们将看到的，它可以。

但为了使其高效，它需要与一个名为反向传播的方法结合使用。

让我们在下一个视频中，看看它的工作原理。

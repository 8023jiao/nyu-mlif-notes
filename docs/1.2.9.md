# 1.2.9 TF 中的线性回归

在之前的视频中，我们熟悉 tensorflow 图，操作，变量和常量。

我们还看到了 tensorflow 如何计算任意函数的梯度。

在本视频中，我们将应用该机制，在 tensorflow 中估计线性回归模型。

我们将使用两种方法来实现：第一种方法是基于我们之前介绍的线性回归的正规方程。

第二种是概率方法，称为最大似然估计或简称 MLE。

所以，让我们深入了解它是如何运作的。

我们在这里用笔记本。

我们常用输入的第一个单元格，将 tensorflow 导入为 tf。

第二个单元格是图的重置工具函数，我们之前笔记本中已经使用了。

第三个单元格定义了我们拟合的线性回归模型。

我们有三个预测器，X1 到 X3，它们从 -1 到 1 均匀采样。

然后我们用权重 B1 到 B3 对它们进行加权，加上截距 A 并最后加上高斯噪声，其波动率 sigma 等于 10%。

接下来，我们模拟 5,000 个这样的整数数据，然后将它以四比一的比例分成训练和测试数据集。

让我们来看看我们在这里得到什么。

好的。

现在，我们的数据准备就绪，我们可以继续估算模型了。

第一个计算显示了如何在普通 numpy 中完成此操作。

我们通过在此处的左侧添加一列来定义扩充数据度量，然后在此处应用正规方程。

好的。

所以，这是结果，到目前为止一切顺利。

所以，现在让我们在 SKlearn 做同样的事情。

现在我们可以简单地编写线性回归 glessine SKlearn 的拟合方法，而不是在这里明确编写正规方程。

我们来吧。

这是结果。

正如你所看到的，它与我们在 numpy 中获得的结果相同，这是因为 Scikit-learn 在背后使用了 numpy。

现在，让我们看看如何使用 tensorflow 完成相同的操作。

在这里，我们再次首先制作一个我们称之为 X_np 的扩充数据矩阵。

然后我们创建两个 tf 节点 X 和 Y，它们将是 tf 常量，将在运行时初始化为 X_7P 和 Y_train。

然后我们在 tf 中实现正规方程。

这次使用 tensorflow 矩阵迭代。

然后，我们让我们的会话来计算 theta 的值，这里是结果。

现在，让我们为线性回归建立一个 tf 类，它实现了正规方程和另一种称为最大似然估计或简称 MLE 的方法。

如果你不熟悉 MLE，请不要担心我们将在本周晚些时候介绍它。

> 线性回归的最大似然（MLE）和最小二乘法（OLS）是一样的，是一个优化目标的不同角度的解释。

因此，如果你不理解我将在 MLE 方法的 tensorflow 中实现的公式，你将能够返回此演示。

好的。

那么，这堂课我们有什么？让我们看一下类构造函数。

这里需要解释的第一件事是这两个属性，self.X 和 self.Y.

它们被声明为占位符。

占位符是 tf 图中的特殊节点，不需要任何计算，只需在运行时传递输入数据。

在创建图形的这个阶段，tensorflow 需要知道，它应该为这些数据创建什么类型的张量。

因此，它的第一个参数将是这些数据的一种类型，第二个参数将是表示此数据的张量的形状。

这里，填充占位符节点的张量形状，为 none 和特征数。

这意味着第二个维度，即数据样本中的列数将是 n 个特征，但第一个维度，数据样本中的点数可以是任何值。

接下来的两行实现了正规方程，如上所述，以及均方误差。

让我们转到这个类中实现的第二个方法。

使用 MLE 方法，我们估计回归的所有参数加噪声方差。

估计需要 n 个特征加上两个参数。

因此，我们创建一个变量 self.weights，它将存储我们的 MLE 估计结果。

接下来的几行计算线性回归的负对数似然函数。

请注意我在这里使用的一个小技巧，将标准差或波动率 sigma 定义为最后一个权重的平方。

加上一些小的正数。

这样做是为了确保此处定义的对数似然值，对于所有权重值保持良好定义。

最后，构造函数中的最后一行定义了一个节点，该节点指定将执行的优化方法。

Tensorflow 拥有许多内置优化器。

我们将使用一个称为 Adam 优化器的方法。

另一种选择可以是梯度下降优化，这是在这里指出的。

稍后我们将讨论这些算法在理论上如何工作，但现在，我们正在研究它们如何在这些简单示例的实践中工作。

该类的其余部分是一个函数，它生成人造数据，为方便起见，我在上面的单元格中复制了这些数据。

现在，这里的主函数首先创建数据，然后创建训练测试数据集。

然后它在这里创建我们的线性回归模型，并启动 tensorflow 会话。

现在，请注意我们如何运行图。

要计算在我们的类中定义的节点，我们需要填充我们在那里创建的占位符。

这是在运行时使用名为`feed_dict`的字典完成的，其键是占位符的节点，值是模型输入。

这样的计算用于计算它们的最佳参数值，训练和测试误差。

请注意，在最后两次计算中，我们计算相同的节点，但每次使用不同的数据，以不同方式填充其占位符。

这是针对类中的正规方程实现。

下一行显示了 MLE 方法的训练。

在这里，我们将进行 1,000 步优化，这将根据我们在类中定义的最小化方案，持续更新模型权重。

此代码返回损失的当前值和更新的模型权重。

训练完成后，我们测试模型。

之后，我们计算噪声波动率的模型预测并打印结果，并显示我们的预测结果的三维投影。

让我们执行这个单元格，并执行下一个单元格来查看结果。

以下是结果。

正如你所看到的，这两种方法都能产生非常相似的结果，并且都能为数据提供良好的输入。

这个笔记本在下面包含另一个线性回归的实现，这次使用不同的算法代码，随机梯度下降，并将线性回归实现为单个神经元神经网络。

所有这些都将成为我们下一个视频的主题。

在熟悉这些主题之后，你可以稍后再回到这个笔记本，看看它们是如何实现的。

现在，让我在此时停止使用笔记本。

好的。

现在，在我们看到 tf 中的线性回归，并使用正规方程解和最大似然解之后，我们准备继续进行机器学习中的回归问题。

在下一个视频中，我们将看看如何使用神经网络解决回归问题。

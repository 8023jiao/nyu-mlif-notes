# 1.3.1 神经网络

我们刚刚进行了TensorFlow的第一个演示。

这看起来很简单，对吧？我们将看看 TensorFlow 如何创建图形并对其进行评估。

然后我们在 TensorFlow 中实现了线性回归，并将其与心理学中的线性回归进行了比较。

而现在，我有一些惊喜要给你。

令人惊讶的是，事实上，我们刚刚实现的是一类非常普遍的神经网络模型的极限情况。

换句话说，重申一下，线性回归是一种非常特殊的神经网络。

因此，我们可以说你刚刚在 TensorFlow 中实现了第一个称为线性回归的神经网络。

现在让我更详细地解释一下我的意思，以及为什么这在实践中有用。

让我们再看看线性回归，并将其视为函数变换。

变换采用实值输入 x，使用权重集 W 计算其线性组合，然后添加截距 W0，最后添加高斯噪声 epsilon 来获得最终输出y。

我们可以将其视为输入的线性变换，它由一组参数 W 控制。

可以调整这些参数来拟合你的训练数据。

我们可以将这种线性变换表示为一种像这样的计算节点。

当我们的节点有“n”个的实值输入时，另外，我在这里添加了一个常数输入 x0 等于 1，这样我们就可以在 x 的总和中包含截距 w0，它在从 0 到 n 的 i 上运行。

此节点的输出只是输入线性回归方程的，输入的线性组合。

我们可以用更一般的形式，将相同的东西写为整个表达式的函数 f。

在我们的线性回归的特定情况下，该函数是单位函数`f(z) = z`。

在更一般的情况下，当它不是单位函数时，该函数有一个名称，它被称为激活函数。

现在如果我们使这个函数更复杂，即非线性。

在这种情况下，我们得出非线性回归。

与前一种情况的唯一区别在于，现在节点的输出是输入的线性组合的一些非线性变换 f。

这种函数将被称为非线性函数功能。

S 形函数是非线性激活函数的一个示例。

sigmoid 函数由此表达式定义，作为其参数的函数，它的行为在此处显示。

对于负参数，sigmoid 很快接近零，并且对于正参数快速接近 1。

在这个专项中我们将多次需要这个函数，但是现在，我只是想用它来说明，非线性变换可以由这样的节点计算。

现在，我们的节点变得更加通用，可以对其输入进行非线性变换。

这种结构有一个名字，它被称为人工神经元。

人造神经元或感知器 由Frank Rosenblatt 于 1957 年引入，作为人类大脑中真实物理神经元功能的极端仿生模型。

物理神经元拥有一个，有时是大量轴突，传递电信号作为其输入，并将其产生的信号传递给其他神经元。

以类似的方式，人工神经元获取其输入并产生一个输出到其他地方。

现在你看到我们在这里调用激活函数 f 的原因了。

它是一个函数，定义了这个人工神经元的活动或触发率的。

实际上，从这一点开始，我们只谈论人工神经元，而不是物理神经元。

因此，为简单起见，当我们指的是人工神经元时，我们只是说神经元。

因此，神经元可以具有不同的激活函数。

例如，这里显示的 sigmoid，tanh 函数或所谓的整流线性（ReLU）激活函数，`max(z, 0)`。

最有趣的见解来自这里。

如果我们向神经元的输入发送一些数据转换，我们可以称之为`h(x)`，而不是原始数据 X，会发生什么？在这种情况下，输出变为复合非线性。

也就是说，由于函数 h 和非线性激活函数 f，它将是非线性的。

这种非线性复合函数可以提供非常复杂的输入分布，因为通过正确选择函数 h 和 f，它可以非常灵活。

好的，到目前为止一切顺利。

但是我们如何进行这些变换呢？这里的答案听起来非常简单，但一旦完全暴露其逻辑结果，它的价值就达数十亿美元。

答案是这样的，让我们使用我们刚刚制作的相同类型的神经元，来产生这样的变换，这将我们带入神经网络。

这里显示的是一个前馈神经网络。

前馈网络是一种高度风格化的模型，表示哺乳动物大脑的部分（称为神经皮层）如何处理视觉和音频信号。

它是如何工作的？前馈网络由多层神经元组成，它们仅在一个方向上从左向右传递信号，或从底部传递到顶部。

信号从 x0 到 xn，在这种情况下是三个，我首先复制到网络的输入层。

输入层除了复制输入之外，不会做任何事情。

然后输入传递到隐藏层中的神经元。

让我把它们的输出称为 z，带有一个上标。

隐藏层中的神经元以我们刚才描述的相同方式工作。

也就是说，他们接受输入，转换它们，并将它们传递给最终的神经元。

现在我们将这些最终神经元 z 的输出添加上标 2，来强调该神经元存在于神经网络的第二层。

请注意，我们不将输入层视为真实层，因为它只复制输入而不执行任何其他操作。

因此输出层的上标是 2，而不是 3。

输出层 z^2 的激活函数的选择取决于我们试图解决的问题。

如果你处理回归问题，那么线性激活函数就可以完成这项工作，如果你处理那个分类问题，那么 sigmoid 函数就是你需要的。

但无论是回归还是分类问题，一般结构都将保持不变。

根据你的神经网络试图解决的问题，只需要替换最后一个输出神经元。

这里最重要的想法是，神经网络中从一层到另一层的级联式数据转换的想法。

我们可以进一步扩展它，并说明为什么只有一个隐藏层？事实上，我们可以再添加一个隐藏层，在这种情况下，我们可以获得一个双隐层神经网络。

我们甚至可以继续并拥有两个以上的隐藏层，如三个，四个，五个，七个等等。

任何具有两个以上隐藏层的神经网络（不计算输出层）称为深度神经网络。

深度神经网络被证明对于许多技术应用来说非常强大，例如图像或人脸识别，并且有一些非常好的理由，我们将在本课程后面讨论。

我们将在这个专项中大量谈论深度学习，但是现在请知道，至少在性质上，这种深层网络与具有一个或两个隐藏层的非深层浅层网络没有什么不同。

特别是，深度和浅层神经网络都使用称为梯度下降的优化方法进行训练。

在下一个视频中，我们将看到它是如何工作的。
